\chapter{Panoramica generale sul machine learning}
\bigskip
\medskip
Il campo del Machine Learning (apprendimento automatico) è un ramo dell'intelligenza artificiale che mira a sviluppare algoritmi capaci di apprendere da dati e migliorare le loro prestazioni e capacità nel corso del tempo.\\\\
I termini machine learning (ML) e AI vengono spesso utilizzati insieme e in modo interscambiabile, ma non hanno lo stesso significato. Un'importante distinzione è che, sebbene tutto ciò che riguarda il machine learning rientra nell'intelligenza artificiale, l'intelligenza artificiale non include solo il machine learning.\\
Nell'ambito dell'informatica, questa (ML) è una variante alla programmazione tradizionale nella quale in una macchina si predispone l'abilità di apprendere qualcosa dai dati in maniera autonoma, senza istruzioni esplicite.\\\\
L'obiettivo principe dell'apprendimento automatico è che una macchina sia in grado di generalizzare dalla propria esperienza, ossia che sia in grado di svolgere ragionamenti induttivi.
Questo, dunque, esplora lo studio e la costruzione di algoritmi che possano apprendere da un insieme di dati e fare delle predizioni su questi, costruendo in modo induttivo un modello basato su dei campioni.\\
In questo contesto, per \textbf{generalizzazione} si intende l'abilità di una macchina di portare a termine in maniera accurata esempi o compiti nuovi, che non ha mai affrontato, dopo aver fatto esperienza su un insieme di dati di apprendimento.\\\\ Si prende dunque in considerazione quello che è chiamato come errore di generalizzazione, con cui si analizza quanto la soluzione fornita si discosta da quella reale.
In pratica, si cerca di ottenere una stima del minimo valore dell’errore e, per fare ciò, l’idea è quella di suddividere l’insieme complessivo dei dati in parti differenti.\\\\
Il training set è il sottoinsieme sul quale avviene l’addestramento dell’algoritmo e mediante cui questo riesce poi ad apprendere;\\
Il test set viene invece usato per testare quanto la soluzione finale (su dati nuovi/di test) sia accurata.\\\\
Per riassumere quindi, l’algoritmo di training userà il training set, minimizzerà l’errore sul training e, solo alla fine, verrà testato sul test set.\\
Si assume che gli esempi di addestramento (training) provengano da una qualche distribuzione di probabilità, generalmente sconosciuta e considerata rappresentativa dello spazio delle occorrenze del fenomeno da apprendere.\\La macchina ha il compito di costruire un modello probabilistico generale dello spazio delle occorrenze, in maniera tale da essere in grado di produrre previsioni sufficientemente accurate quando sottoposta a nuovi casi.
\\
\\
\section{Workflow del Machine Learning e introduzione al boosting}
Per quanto riguarda il workflow generico del machine learning si possono distinguere le seguenti fasi:\bigskip
\begin{enumerate}
    \item Raccolta dati;
    \item Data pre-processing;
    \item Ricerca del miglior modello per il tipo dei dati che si stanno considerando;
    \item Training e testing del modello;
    \item Valutazione finale;
\end{enumerate}\bigskip
Analizziamo questi punti più nello specifico:
\bigskip
\begin{enumerate}
    \item La scelta del dataset dipende dal contesto che vogliamo analizzare e dalla tipologia di soluzione che vogliamo ottenere alla fine del processo;
    \item I dati, prima di poter essere elaborati, vanno trasformati mediante la fase di pre-processing in una forma più pulita. Attraverso questa fase, si eliminano informazioni ritenute inutili ai fini dell’analisi, si aggiungono eventuali variabili ritenute necessarie e si modificano i dati che si hanno a disposizione in una forma che faciliti o in certi casi ne renda effettivamente possibile uno studio.\\Accade spesso infatti che nei dati iniziali siano presenti “Missing data” (dati mancanti), “Noisy data” (o rumorosi, sono distorti in qualche modo. Questo può essere dovuto ad esempio anche all’errore umano), “Inconsistent data” (come, ad esempio, dati duplicati o che presentano incoerenze);
    \item La scelta del miglior modello da utilizzare si basa sull’obiettivo che ci si prepone inizialmente e, a seconda dell’algoritmo, si ricade all’interno di una tra diverse categorie, dette paradigmi. Tra questi si identificano:\bigskip
    \begin{itemize}
        \item Apprendimento supervisionato: Ogni campione di dati contiene una coppia input-output di elementi si=(xi,yi). In questo sono presenti le categorie di problemi di Regressione e Classificazione:\bigskip
        \begin{itemize}
            \item La regressione cerca di predire o stimare il valore di una variabile dipendente continua sulla base di una o più variabili indipendenti. Si cerca quindi di creare un modello che possa catturare la relazione tra le variabili di input e la variabile di output continua, consentendo di fare stime accurate su nuovi dati non ancora visti.
            \item La classificazione cerca di assegnare un'etichetta o una categoria a un'istanza di dati in base alle sue caratteristiche. Si concentra quindi sulla categorizzazione di dati in classi distinte.
        \end{itemize}
        \item Apprendimento non supervisionato: Ogni campione di dati contiene solo l’input si=xi. In questo i problemi si categorizzano in Clustering e Associazione:\bigskip
        \begin{itemize}
            \item Il clustering consiste nel raggruppare insiemi di dati simili in cluster o gruppi, in modo che gli elementi all'interno di ciascun cluster siano più simili tra loro rispetto agli elementi in altri cluster.
            \item L’associazione riguarda invece l'identificazione di regole o modelli che indicano la correlazione tra diverse variabili o elementi in un dataset.
        \end{itemize}
    \end{itemize}
    \item Per il training e il testing di un modello nel contesto del machine learning, vengono impiegati diversi metodi, tra i quali si evidenziano in questa Tesi l'Hold-out method e la Cross Validation. In questa fase del workflow, l'obiettivo è quello di addestrare il modello su dati specifici e successivamente valutarne le prestazioni su nuovi dati, stimando così la sua capacità di generalizzazione.\\
    Analizziamo più nel dettaglio queste due tecniche:\bigskip
    \begin{itemize}
        \item HOLD OUT METHOD:\\\\Con questa tecnica il dataset viene suddiviso in tre sottoinsiemi: training set, validation set e test set.
        \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.3]{img/hold_out.jpg}\\
        \caption{Suddivisione dei dati in Hold out method}
        \end{figure}\\\\Identifichiamo a questo punto 2 fasi: la model selection e la model assessement.\\\\
        La fase di model selection mira a trovare la configurazione ottimale dei parametri del modello, noti come iperparametri.\\Gli iperparametri influenzano il comportamento del modello, ma non sono appresi durante la fase di addestramento, pertanto, devono essere regolati manualmente.\\\\
        Per ogni configurazione degli iperparametri associati al modello:\bigskip
        \begin{itemize}
            \item Si costruisce il modello usando i dati di training;
            \item Viene calcolato il validation error associato a tale modello;
        \end{itemize}
        Tra tutte le configurazioni ottenute si mantiene quella col validation error più basso;\\
        La model assessement è la fase successiva alla model selection e si focalizza sulla valutazione delle prestazioni del modello scelto, usando un set di dati separato dai primi due chiamato test set.\\Questa fase è fondamentale per comprendere come il modello selezionato generalizzerà su dati completamente nuovi e sconosciuti.\\In questa:\bigskip
        \begin{itemize}
            \item Si definisce il modello finale l’unione dei dati di training set e validation set;
            \item Si valuta tale modello sul test set.

        \end{itemize}
        \item CROSS VALIDATION:\\\\
        In questa tecnica (anche nota come “k-fold cross-validation”) l’intero dataset viene diviso in una parte relativa ai dati di addestramento e l’altra relativa al test set. Mentre il test set continua ad essere usato per la valutazione finale, i dati di addestramento vengono ulteriormente divisi in k gruppi.
        \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.3]{img/cross_validation.jpg}\\
        \caption{Cross Validation}
        \end{figure}\\\\
        Questo significa che il dataset è diviso in k parti, e in ciascuna iterazione del processo, una di queste parti viene utilizzata come validation set (per il calcolo del relativo validation error), mentre le rimanenti k-1 parti vengono utilizzate per addestrare il modello.\\
        Durante ogni iterazione, il modello viene addestrato sulle k-1 fold e successivamente valutato sulla k-esima fold. Questo processo viene ripetuto finché ogni fold non è stata utilizzata come validation set almeno una volta. I risultati ottenuti su ciascuna iterazione vengono registrati, permettendo una valutazione accurata delle prestazioni medie del modello sui dati considerati.\\\\
        Alla fine del processo, infatti, le metriche di valutazione ottenute in ciascuna iterazione vengono aggregate per fornire una stima complessiva delle prestazioni di una particolare configurazione del modello.\\
        Tutto questo avviene infatti per una singola configurazione di iperparametri. Una volta che questo meccanismo viene eseguito per tutte le configurazioni di iperparametri che si vogliono prendere in considerazione, viene poi utilizzato il test set per comprendere come il modello selezionato generalizzerà su dati completamente nuovi e sconosciuti.
    \end{itemize}

    \item Questa fase aiuta a trovare il modello migliore che rappresenta i dati e a capire quanto bene funzionerà il modello scelto in futuro. Durante la valutazione si prendono in considerazione diverse tipologie di errori, tra cui il Training error ed il test error. Il primo è ciò che il nostro algoritmo vuole minimizzare per cercare di trovare la soluzione migliore. Il test error invece rappresenta un valore che resta fisso e che rappresenta l’errore ottenuto durante l’applicazione del modello a dati sconosciuti.\\In base ai valori di questi errori si possono ottenere diverse situazioni:\bigskip
    \begin{figure}[ht]
    \centering
    \includegraphics[scale=0.1]{img/overf_vs_underf.jpg}\\
    \caption{Cross Validation}
    \end{figure}\\\\
    \begin{itemize}
        \item Training error e test error alti -> Underfitting;
        \item Training error e test error bassi -> Obiettivo;
        \item Training error basso e test error alto -> Overfitting;
        \item Training alto e test error basso -> Impossibile;

    \end{itemize}
    Affinché la generalizzazione offra le migliori prestazioni possibili, la complessità dell'ipotesi induttiva deve essere pari alla complessità della funzione sottostante i dati. Se l'ipotesi è meno complessa della funzione, allora il modello manifesta underfitting. Quando la complessità del modello viene aumentata in risposta, allora l'errore di apprendimento diminuisce. Al contrario invece se l'ipotesi è troppo complessa, allora il modello manifesta overfitting e la generalizzazione sarà più scarsa.
\end{enumerate}\bigskip
Dovendoci addentrare verso l’algoritmo XGBoost diamo dunque maggiore importanza all’aspetto della classificazione.\\
Nel contesto del paradigma del supervised learning consideriamo un insieme di dati di addestramento $ D=	\left \{ (x1,y1), (x2,y2), …, (x3,y3) \right \}$ dove le $x_i$ sono variabili/caratteristiche e le $y_i$ le etichette associate. Un modello di machine learning lo possiamo vedere come una funzione f che mappa un input x a una previsione $\hat{y}$. La situazione ideale è definita dal caso $y=f(x)+\epsilon$ (con $\epsilon$ che rappresenta l’errore relativo al fit della funzione f (funzione vera), ovvero è un errore di misura). L’approssimazione invece è $\hat{y}$ che fa invece riferimento alla stima del modello che chiameremo $\hat{f}$.\\
L'errore di previsione di un modello può essere scomposto in tre componenti principali: il quadrato del bias, la varianza e l'errore irriducibile.\\ Questa decomposizione è nota come scomposizione bias-varianza. La somma di questi tre componenti rappresenta l'errore totale del modello.\\Quindi, matematicamente, possiamo esprimere l'errore totale (E) come la somma del quadrato del bias (B), della varianza (V), e dell'errore irriducibile (I):\\\\
$E=B^2 + V + I$\\\\
Dove:\bigskip
\begin{itemize}
    \item B, rappresenta il bias del modello, che misura quanto il modello medio stimato si discosta dal valore atteso corretto.
    \item V, rappresenta la varianza del modello, che misura quanto le predizioni del modello variano rispetto al modello medio.
    \item I, rappresenta l'errore irriducibile, che è l'errore intrinseco che non può essere eliminato da nessun modello. È causato da fattori non controllabili o dalla presenza di rumore nei dati.

\end{itemize}

\chapter{Storia e predecessori di XGBoost}
Prima di parlare nello specifico di XGBoost e delle sue caratteristiche è importante darne una visione dal punto di vista storico, spiegando anche il perché questo sia stato introdotto.\\
Questo algoritmo è infatti fortemente legato al concetto di alberi decisionali, che sono in questo ambito chiamati come alberi di boosting. Si usa infatti un ensemble di alberi decisionali per migliorare le prestazioni predittive.\\
Come vedremo, l’idea è quella di fare in modo che ogni albero sia costruito successivamente per focalizzarsi sugli errori residui degli alberi precedenti, fornendo un modello finale più accurato e robusto. Partiamo dunque dalla definizione di questa struttura preliminare.\\

\section{Alberi decisionali}
Un albero decisionale è un algoritmo di apprendimento supervisionato non parametrico, utilizzato sia per attività di classificazione che di regressione, e sono ampiamente usati in diversi campi di apprendimento automatico e della statistica.\\
Questo si compone di una struttura ad albero gerarchica, che consiste di un nodo radice, nodi interni, nodi foglia e di rami (relazioni tra i nodi).\\\\
Più nello specifico:\bigskip
\begin{itemize}
    \item Radice: È il punto di partenza dell'albero, dove inizia il processo decisionale. Questa è il primo nodo che effettua un test su una variabile specifica;
    \item Nodi interni: Questi sono punti di decisione. Ogni nodo interno rappresenta un test su una caratteristica, e il risultato del test guida il flusso delle decisioni verso altri nodi interni o foglie;
    \item Foglie: Sono i punti finali dell'albero.
    Ogni foglia fornisce una previsione o una classificazione finale. Non ci sono ulteriori decisioni da prendere una volta che si raggiunge una foglia;
    \item Rami: Sono i collegamenti tra i nodi. Ogni nodo interno ha rami che conducono a nodi successivi o foglie, a seconda del risultato del test. I rami rappresentano il flusso delle decisioni lungo la struttura ad albero.

\end{itemize}
Nel machine learning un albero di decisione è un modello predittivo, dove ogni nodo interno rappresenta una variabile, un arco verso un nodo figlio rappresenta un possibile valore per quella proprietà e una foglia il valore predetto per la variabile obiettivo a partire dai valori delle altre proprietà, che nell'albero è rappresentato dal cammino (path) dal nodo radice (root) al nodo foglia.\\
La ripartizione dei dati verso i nodi sottostanti alla radice avviene in maniera ricorsiva mediante un predicato che si associa ad ogni nodo interno. Tale predicato è chiamato condizione di split.\\
L’albero infatti viene costruito con un processo ricorsivo tale per cui, ad ogni passo, da un nodo interno (padre) vengono definiti due nodi figli. Tale metodo viene anche definito segmentazione binaria, in quanto l’idea di base è partizionare un insieme di unità statistiche in gruppi sempre meno numerosi e sempre più omogenei.\\
\bigskip
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/dec_tree1.jpg}\\
\caption{Struttura generale albero decisionale}
\end{figure}\\\\
Ogni split si basa sull’idea che la complessità del nodo attuale venga via via migliorata/ridotta durante la creazione dei livelli sottostanti nell’albero.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/dec_tree2.jpg}\\
\caption{Esempio di immissione regole}
\end{figure}\\\\
Nell’immagine sopra mostrata ad esempio si cerca di identificare, nell’ambito del football americano, se un giocatore potrà fare touchdown o se verrà probabilmente intercettato.\\\\
Tra i vantaggi dell’utilizzo degli alberi decisionali troviamo:\bigskip
\begin{itemize}
    \item Facilità di comprensione;
    \item Ottima gestione di diverse tipologie di dati (sia quantitativi che qualitativi);
    \item Capacità di fornire un ranking per l’importanza delle variabili (in base a quante volte sono state usate per fare gli split nei nodi);
    \item Velocità di utilizzo;
\end{itemize}
Tra gli svantaggi invece si trovano:
\begin{itemize}
    \item Overfitting: Questi hanno la tendenza a adattarsi eccessivamente ai dati di training, memorizzandoli invece di generalizzarli. Ciò può portare a una scarsa capacità di fare previsioni accurate su nuovi dati, ovvero ad un’alta accuratezza in training ma una bassa accuratezza in test;
    \item Sensibilità ai piccoli cambiamenti nei dati: Un piccolo cambiamento nei dati di training può portare a un cambiamento significativo nella struttura dell'albero decisionale. Questa sensibilità può comportare una scarsa stabilità nelle previsioni;
\end{itemize}
I due svantaggi citati possono portare gli alberi decisionali ad avere limitate capacità di generalizzazione, specialmente su problemi complessi. La costruzione di un albero singolo potrebbe infatti non essere sufficiente per catturare la complessità dei dati.
\subsection{Rappresentazione geometrica analoga}
Dal punto di vista geometrico si può pensare agli alberi decisionali come ad un insieme di iperpiani paralleli ad assi cartesiani che suddividono lo spazio durante il processo di classificazione/costruzione dell’albero.
\\Più nello specifico l’idea è quella di dividere lo spazio in regioni; quindi, gli alberi di decisione dividono lo spazio degli input con split ricorsivi binari:\bigskip
\begin{itemize}
    \item Si parte da una regione R;
    \item Si seleziona una variabile di split j (feature per cui splittare) e uno splitting point s (valore di split);
    \item Si divide la regione $R$ in due regioni $R_l$ ed $R_r$:\bigskip
    \begin{itemize}
        \item $R_l=\{ x | x_j \le s \cap x \in \mathbb{R} \}$
        \item $R_r=\{ x | x_j > s \cap x \in \mathbb{R} \}$
        \item $R_l \cup R_r$
    \end{itemize}
\end{itemize}
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/alberi_cartesiani.jpg}\\
\caption{Rappresentazione spaziale degli alberi decisionali}
\end{figure}
Una rappresentazione di questo tipo ci fa comprendere anche una delle caratteristiche vantaggiose dell’utilizzo degli alberi decisionali. Ovvero, questi non si limitano solo a dividere in due lo spazio, ma consentono di delimitare sezioni precise nello spazio e di svolgere un buon tipo di classificazione.\\
La classificazione avviene infatti in base a quale sezione di piano appartiene ciascun punto considerato.\\
\subsection{Modalità di fine costruzione}
Se si continua a far crescere l’albero fino al punto in cui ogni nodo corrisponde all’impurità più bassa possibile si tenderà ad avere una situazione di overfitting. Se, al contrario, la costruzione si interrompe troppo presto, non si avrà una classificazione abbastanza dei vari dati e si avrà una situazione di underfitting.\\\\
Per prevenire overfitting ed underfitting va allora data importanza al criterio col quale terminerà la costruzione. Questo può essere fatto in due modi:\bigskip
\begin{enumerate}
    \item Settare dei vincoli sulla dimensione dell’albero;
    \item Pruning dell’albero;

\end{enumerate}
Più nello specifico:\bigskip
\begin{enumerate}
    \item \textbf{Settare dei vincoli sulla dimensione dell’albero:}\\\\
    Per fare questo ci sono diversi modi, come:\bigskip
    \begin{itemize}
        \item Fornire un numero minimo di campioni per la suddivisione di ciascun nodo;
        \item Distribuire il numero minimo di campioni per ciascuna foglia;
        \item Definire un valore massimo per la profondità dell’albero;
        \item Definire un numero massimo di foglie;
        \item Definire un numero massimo di feature utilizzabili;
    \end{itemize}
    \item \textbf{Pruning dell'albero:}\\\\
    Questa è una tecnica di machine learning che riduce la dimensione degli alberi decisionali rimuovendo sezioni dell'albero. Riduce inoltre la complessità del classificatore finale e quindi migliora l'accuratezza predittiva riducendo l'overfitting.\\
    Esistono due modalità di pruning:\bigskip
    \begin{enumerate}
        \item \textbf{Pre-pruning:} In questa il pruning viene svolto durante la costruzione dell’albero. Quando ci si accorge che la complessità di un nodo non viene migliorata dallo split avviene la potatura e si stoppa la costruzione dell’albero lungo quel percorso. Oltre alla complessità può considerare anche altre due misure per decidere di fermare la costruzione, come:\bigskip
        \begin{enumerate}
            \item Se il numero di punti/dati è inferiore rispetto ad una certa soglia;
            \item Se la profondità dell’albero raggiunge una soglia preimpostata;
        \end{enumerate}
        \item \textbf{Post-pruning:} Viene svolta consentendo prima all’albero di crescere completamente e si raffina dal basso verso l’alto eliminando i sottoalberi che hanno una complessità più alta di quella della loro radice.
    \end{enumerate}
\end{enumerate}
Tra le due, la tecnica più efficace a livello di risultati è quella del post-pruning, perché una volta costruite tutte le possibili combinazioni si passa a raffinare più precisamente l’albero.
Tuttavia, questa, proprio per la costruzione intera dell’albero, ha un’elevata complessità computazionale e, proprio per questo motivo, per grossi dataset non è ottimale da utilizzare in termini temporali.

\chapter{Apprendimento d'insieme - Ensemble}
Dopo aver esplorato gli alberi decisionali come modelli di apprendimento automatico, è fondamentale considerare le sfide che possono emergere con l'uso di alberi decisionali singoli. Come discusso nel capitolo precedente, tali modelli possono soffrire di overfitting, sensibilità ai piccoli cambiamenti nei dati, e limitate capacità di generalizzazione.\\
Per affrontare queste sfide e migliorare ulteriormente le prestazioni dei modelli, l’idea proposta dai ricercatori è stata quella di esplorare l'apprendimento d'insieme, una metodologia che sfrutta il potere della collaborazione tra modelli.\\
In particolare, con il termine Ensemble (o ensemble learning) si intende la combinazione di più modelli, ciascuno addestrato su porzioni diverse dei dati o con approcci diversi, al fine di ottenere previsioni più accurate e robuste.\\
Utilizzando più alberi decisionali sugli stessi dati si riduce la probabilità di errore, in quanto ogni albero segue un metodo di ragionamento differente. Proprio per il fatto che i ragionamenti usati sono diversi, se si ottiene lo stesso risultato più volte aumenta la probabilità che questo sia corretto.\\
Infatti, l’idea è quella di consultare i risultati ottenuti dai vari alberi decisionali e di scegliere come risposta quella fornita dalla maggior parte di questi.\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/intro_ensemble.jpg}\\
\caption{Principio dell'Ensemble}
\end{figure}\\\\
Solitamente il singolo modello che viene combinato agli altri è detto weak learner (questo è un modello leggermente migliore di un modello casuale), e le soluzioni di più weak learners danno origine ad un modello con performance migliori, chiamato strong learner (o ensemble model).\\
Due tra i modi più usati con cui è possibile combinare weak learners sono il Bagging ed il Boosting (del quale vedremo che farà poi parte l’algoritmo XGBoost).
\section{Bagging}
L’idea del Bagging (Bootstrap Aggregating) è quella di considera spesso weak learners omogenei e combinarli in maniera parallela.\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/bagging_jpg.jpg}\\
\caption{Principio del Bagging}
\end{figure}\\\\
Gli N dati iniziali vengono prima usati per generare dei campioni casuali di dimensione B con la tecnica Bootstrap (questi sono detti bootstrap samples). Ognuno di questi viene generato in modo casuale con rimpiazzamento.\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/bootstrap_2.jpg}\\
\caption{Bootstrap}
\end{figure}\\\\
Una tra le tecniche più famose di questa tipologia di algoritmi è il Random Forest, che considera un insieme di alberi decisionali (foresta) e tende a combinarli tra di loro. Essendo utilizzata inizialmente la tecnica Bootstrap per campionare i dati, che utilizza l’estrazione con rimpiazzamento, possono esserci anche più volte gli stessi dati in un singolo albero.\\
Alla fine, questi centinaia/migliaia di alberi vengono combinati tra loro e portano ad una soluzione finale più accurata.
\section{Boosting}
Spesso considera weak learners omogenei e svolge l’apprendimento sequenzialmente in modo adattivo (ogni modello base dipende dai precedenti). Questi vengono combinati mediante una strategia deterministica. I vari modelli vengono addestrati in sequenza, e l'attenzione viene focalizzata sugli errori fatti dai modelli precedenti. Ciascun modello cerca di correggere gli errori del modello precedente, concentrandosi su istanze difficili o mal predette.\\
In particolare, inizialmente viene creato un primo modello a partire dai dati di training; Il secondo modello invece viene costruito cercando di correggere gli errori presenti nel primo modello, e così via. Questa procedura viene continuata e i modelli vengono aggiunti finchè non viene previsto correttamente il set completo di dati di training, o finchè non viene aggiunto il massimo numero di modelli.\\
Dunque, i vari modelli sono addestrati in sequenza, e ciascuno cerca di migliorare le prestazioni del modello complessivo. Questo processo di aggiornamento graduale consente al modello complessivo di migliorare progressivamente.\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/boosting_jpg.jpg}\\
\caption{Bootstrap}
\end{figure}\\\\
Lo pseudocodice degli algoritmi di Boosting è il seguente:\bigskip
\begin{enumerate}
    \item Inizializza il dataset ed assegna ai dati pesi uguali;
    \item Fornisci questi dati al modello e identifica i data points classificati male;
    \item Incrementa il peso dei data points classificati male e decrementa quello dei dati classificati correttamente. Poi normalizza i pesi di tutti i data points.
    \item if (si ottengono i risultati voluti)
        \begin{itemize}
            \item Vai allo step 5;
        \end{itemize}
        else
        \begin{itemize}
            \item Torna allo step 2;
        \end{itemize}
        
    \item End.

\end{enumerate}
\section{Evoluzione degli algoritmi di Boosting fino a XGBoost}
L’idea del Boosting è stata introdotta nel 1990 da Robert Schapire. La prima tecnica che ha introdotto l’idea del Boosting è stata AdaBoost (Adaptive Boosting) e fu proposta nel 1995 da Yoav Freund e Robert Schapire. Questa fu una delle prime tecniche a dimostrare quelli che erano i reali vantaggi dell’Ensemble.\\
Il modello complessivo in questo caso viene definito mediante una combinazione lineare dei modelli deboli:$\\\\$
$H(x)=sign(\sum_{t=1}^{T}\alpha_t h_t(x)$
$\\\\$
Dove:\bigskip
\begin{itemize}
    \item $h_t(x_i)$ è la predizione del modello t sulla i-esima istanza;
    \item T è il numero totale di modelli deboli;
    \item sign(…) restituisce il segno del suo argomento;
    \item $\alpha_t$ è il peso assegnato al modello debolmente predittivo $h_t$;

\end{itemize}
Il principale svantaggio di AdaBoost è che necessita di un dataset di qualità; quindi, dati rumorosi e valori anomali devono essere evitati prima di utilizzarlo. Essendo molto suscettibile al rumore è facilmente soggetto a situazioni di overfitting.\\
Per superare le sue limitazioni si è arrivati allo sviluppo del Gradient Boosting. Questo è stato pensato per la prima volta nel 2001 da Jerome Friedman e ne sono poi state sviluppate diverse varianti negli anni successivi.\\
Gradient Boosting, in alcuni casi può offrire prestazioni migliori su grandi dataset rispetto a AdaBoost, grazie a implementazioni ottimizzate e strategie di apprendimento più efficienti.\\
\subsection{Spiegazione Teorica Gradient Boosting}
Come in ogni algoritmo supervisionato, Gradient Boosting cerca di trovare la miglior funzione che minimizza l’expected loss tra le labels reali e le previsioni date da F:\\\\
$\hat{F}(x)=\arg\min_{F} \mathbb{E}_{x,y}[L(y,F(x))]$
\\\\
Nel Gradient Boosting si modella la funzione F prendendo un valore costante che può essere considerato in una base di qualche tipo, e poi si ha un multiple weighted weak learner che ha il ruolo di far avvicinare le previsioni ai valori delle labels reali y.\\\\
$\hat{F}(x)=\sum_{m=1}^{M}\gamma_m h_m(x)+const$
\\\\
Gamma è il peso assegnato al weak learner $h_m$. Il valore costante const è la nostra miglior ipotesi iniziale e viene denominata come $F_0$:
\\\\
$\hat{F}_0(x)=\arg\min_{\gamma}\sum_{i=1}^{n}L(y_i, \gamma)$
\\\\
Questa viene calcolata come il valore che minimizza la Loss function scelta.\\
I weak learners vengono invece creati guardando ciò che si dovrebbe avere nella funzione attuale al fine di migliorare la previsione e minimizzare la Loss.\\\\
$\hat{F}_m(x)=F_{m-1}(x)+(\arg\min_{h_m \in \mathbb{H}}[\sum_{i=1}^{n}L(y_i, F_{m-1}(x_i)+h_m(x_i))])(x)$
\\\\
$F_{m-1}$ è l’approssimazione corrente che si cerca di migliorare per ottenere $F_m$.\\
Per fare ciò si pensa ai weak learner $h_m$ che, quando vengono aggiunti ad $F_(m-1)$ minimizzano la Loss tra la previsione e le labels.\\
Sfortunatamente non è computazionalmente possibile trovare i weak learner ottimali in questa equazione, e il meglio che si può fare è svolgere un ulteriore passo per questo problema di ottimizzazione. L’idea è quella di adottare una strategia di ottimizzazione iterativa nota come discesa del gradiente. Questa tecnica consente di cercare il minimo locale della funzione di perdita, iterando sui modelli già costruiti fino a quel momento, indicati come $F_{m-1}(x)$.\\
La discesa del gradiente sfrutta il concetto che la direzione di massima diminuzione della Loss è data dal gradiente negativo. In altre parole, si cerca di muoversi nella direzione opposta al gradiente della funzione di perdita per ridurre progressivamente l’errore del modello. Questo processo è iterativo, e ad ogni passo si aggiorna il modello $F_{m-1}$ con un nuovo weak learner, correggendo così le previsioni precedenti. La discesa del gradiente consente di affrontare il problema di ottimizzazione in maniera efficiente, migliorando iterativamente la precisione del modello complessivo.
\\
Si considera dunque il gradiente, dal quale prende anche il nome l’algoritmo, ottenendo:\\\\
$\hat{F}_m(x)=F_{m-1}(x)-\gamma\sum_{i=1}^{n}\nabla_{F_{m-1}}L(y_i, F_{m-1}(x_i))$
\\\\
con $\gamma>0$
\\\\
Dal punto di vista algoritmico, nel Gradient Boosting, i passi che vengono seguiti sono i seguenti:\bigskip
\begin{itemize}
    \item Inizializzazione:
    \begin{itemize}
        \item Inizializza il modello $F_0(x)$ con una stima iniziale (spesso la media delle risposte osservate);
    \end{itemize}
    \item Per ogni iterazione t:
    \begin{itemize}
        \item Calcola i residui $r_it=y_i-F_{t-1}(x_i)$ per ogni istanza i nel dataset, dove $y_i$ è l’etichetta reale.
	\item Addestra un modello debolmente predittivo $h_t(x)$ per predire i residui $r_it$.
	\item Determina un coefficiente di apprendimento $\alpha_t$ che regola la contribuzione del modello debolmente predittivo.
        \item Aggiorna il modello complessivo $F_t (x)=F_{t-1}(x)+\alpha_t h_t(x)$.

    \end{itemize}
    \item Predizione Finale:
    \begin{itemize}
        \item La predizione finale è data da $F_t (x)$, dove $T$ è il numero totale di iterazioni.
    \end{itemize}
\end{itemize}
Dunque, AdaBoost si concentra sull'aggiornamento dei pesi per enfatizzare le istanze mal classificate, mentre Gradient Boosting si concentra direttamente sulla correzione dei residui delle previsioni del modello precedente. AdaBoost utilizza modelli deboli ponderati (solitamente alberi decisionali poco profondi), mentre Gradient Boosting può utilizzare modelli più complessi, spesso alberi decisionali profondi. AdaBoost può essere più sensibile agli outliers rispetto a Gradient Boosting, che può trattare meglio i valori anomali attraverso l'addestramento sequenziale di modelli.
\chapter{XGBoost}
Partendo dagli alberi binari abbiamo visto tutta l’evoluzione storica che è stata necessaria per l’ideazione degli algoritmi di Boosting più sviluppati.\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{img/evoluzione_dec_tree_xgboost.jpg}\\
\caption{Evoluzione dei modelli dagli alberi di decisione ad XGBoost}
\end{figure}\\\\
Dopo aver esplorato le fondamenta del gradient boosting e compreso il suo potenziale nel migliorare iterativamente i modelli predittivi, ci concentriamo ora su un algoritmo che ha rivoluzionato il campo dell'apprendimento automatico: XGBoost.\\
Questo algoritmo ad oggi risulta essere quello più utilizzato nell’ambito del Boosting.\\\\
XGBoost, acronimo di “eXtreme Gradient Boosting”, è una potente estensione del gradient boosting che ha dimostrato di eccellere in termini di precisione predittiva, velocità di addestramento e robustezza.\\
Fu introdotto da Tianqi Chen nel 2006 ed è diventato uno degli algoritmi più ampiamente utilizzati in competizioni di data science e applicazioni del mondo reale.
\\
Questo diventò famoso dopo aver permesso di vincere la Higgs Machine Learning Challenge di Kaggle, che aveva lo scopo di esplorare le potenzialità dei metodi avanzati di "machine learning" per migliorare il potenziale di scoperta degli esperimenti di fisica delle particelle elementari.\\
Più in particolare il compito dei partecipanti era quello di classificare al meglio gli eventi nelle categorie di segnale e di fondo.
\section{Fondamenti teorici di XGBoost}
Trattandosi dunque di un’estensione del Gradient Boosting, XGBoost integra alcuni concetti nuovi. Dunque, la parte teorica vista precedentemente resta valida anche in questo caso.\\
In questo caso, per ogni albero $m=1,…,M$, si calcolano la matrice dei gradienti ed Hessiana:\\\\\\
$\hat{g}_m(x_i)=[\frac{\partial(L(y_i, f(x_i)))}{\partial f(x_i)}]_{f(x)=\hat{f}_{m-1}(x)}$
\\\\\\
$\hat{h}_m(x_i)=[\frac{\partial(L(y_i, f(x_i)))}{\partial^2 f(x_i)}]_{f(x)=\hat{f}_{m-1}(x)^2}$
\\\\\\
I gradienti, spesso definiti “pseudo-residui”, mostrano come cambia la loss function per il cambio di un’unità nel valore delle $x_i$. L’Hessiana (derivata del gradiente) definisce di quanto cambia la loss in base alle $x_i$.\\
Usando queste matrici l’idea è quella per cui il nuovo albero da aggiungere segue, per ogni iterazione dell’algoritmo, il seguente problema di ottimizzazione (che usa un’approssimazione di Taylor):\\\\
$\hat{\phi}_m=\arg\min_{\phi \in \Phi}\sum_{i=1}^{N}\frac{1}{2}\hat{h}_m(x_i)[-\frac{\hat{g}_m(x_i)}{\hat{h}_m(x_i)}-\phi_m(x_i)]^2$
\\\\
Dal quale si ottiene poi il nuovo albero che viene aggiunto al modello, definito in:\\\\
$\hat{p}_m(x)=\alpha \hat{\phi}_m(x)$
\\\\
La quantità $\frac{g}{h}$ è utilizzata per ottenere un valore che riflette la quantità di "correzione" necessaria alla previsione del modello corrente. L'idea è che, se il gradiente è grande (indicando una grande discrepanza tra la previsione attuale e la risposta effettiva) e l'hessiano è piccolo (indicando una curva relativamente piatta), allora è necessario fare un aggiustamento significativo.\\
$\alpha$ è il learning rate inerente al problema di ottimizzazione e determina di quanto cambia il modello. Più è alto e più la passata iterazione avrà influenza su quella nuova.\\
La formula definita per alpha è la seguente:\\\\
$\alpha_t = \frac{-\sum_{i=1}^N g_{it}+\lambda}{\sum_{i=1}^N h_{it}+\lambda}$
\\\\
\begin{itemize}
    \item $\sum_{i=1}^N g_{it}$ rappresenta la somma dei gradienti su tutte le istanze.
    \item $\sum_{i=1}^N h_{it}$ rappresenta la somma degli hessiani su tutte le istanze.
    \item $\lambda$ è il termine di regolarizzazione.

\end{itemize}

Il modello aggiornato risulta dunque essere questo:\\\\
$\hat{f}_m(x)=\hat{f}_{m-1}(x)+\hat{p}_m(x)$
\\\\
Come discusso prima, i weak learners vengono usati in modo che il modello acquisisca precisione nel tempo in base al processo di minimizzazione della Loss.\\
Quindi l’output finale di XGBoost si può esprimere come la somma di ogni singolo weak learner:\\\\
$\hat{f}(x)=\hat{f}_M(x)=\sum_{m=0}^M \hat{f}_m(x)$
\\\\
Dal punto di vista algoritmico, i passi seguiti in XGBoost sono i seguenti:\bigskip
\begin{itemize}
    \item Inizializzazione:
    \begin{itemize}
        \item Si inizia con un modello iniziale semplice, spesso impostato su una stima come la media delle risposte osservate;
    \end{itemize}
    \item Per ogni iterazione t:
    \begin{itemize}
        \item Calcolo dei residui:
        \begin{itemize}
            \item Si calcolano i residui come la differenza tra le risposte effettive e le previsioni correnti del modello:\\\\
            $r_{it}=y_i-F_{t-1}(x_i)$
            \item Dove $y_i$ è la risposta effettiva e $F_{t-1}(x_i)$ è la previsione del modello al passo $t-1$.
        \end{itemize}
        \item Addestramento dell’albero decisionale:
        \begin{itemize}
            \item Addestra un nuovo albero decisionale per predire i residui $r_{it}$.
        \end{itemize}
        \begin{itemize}
            \item Determinazione del Coefficiente di Apprendimento $\alpha_t$:
            \begin{itemize}
                \item Calcola il coefficiente di apprendimento ($\alpha_t$) che pesa l’aggiunta del nuovo albero al modello complessivo. Questo può essere fatto risolvendo un problema di ottimizzazione per minimizzare la funzione di perdita.
            \end{itemize}
            \item Aggiornamento del modello complessivo:
            \begin{itemize}
                \item Aggiorna il modello complessivo con la nuova previsione ponderata dal coefficiente di apprendimento.\\\\
                $F_t(x)=F_{t-1}(x)+\alpha h_t(x)$\\
                Dove $h_t(x)$ è la previsione del nuovo albero.
            \end{itemize}
            \item Iterazione:
            \begin{itemize}
                \item Ripeti i passi precedenti fino a raggiungere il numero prefissato di iterazioni o finchè il modello non converge.
            \end{itemize}
            \item Previsione Finale:
            \begin{itemize}
                \item La previsione finale è data dalla somma delle previsioni di tutti gli alberi.\\\\
                $F_t(x)=\sum_{t=1}^T \alpha_t h_t(x)$\\\\
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}
\section{Caratteristiche di XGBoost}
Vediamo adesso la sezione inerente alle caratteristiche dell'algoritmo. La figura sottostante le riassume bene:\\\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{img/caratteristiche_xgboost.jpg}\\
\caption{Caratteristiche dell'algoritmo XGBoost}
\end{figure}
Per quanto riguarda l’ottimizzazione di sistema si hanno le seguenti caratteristiche:\bigskip
\begin{itemize}
    \item \textbf{Tree Pruning:} L’algoritmo XGBoost utilizza l’approccio depth-first. Dunque, prima costruisce l’albero con la sua profondità massima e poi lo pota dal basso verso l’alto.
    \item \textbf{Parallelizzazione:} La logica del Boosting prevede, come visto prima, che il miglioramento dello strong model avvenga in maniera sequenziale integrando uno ad uno i vari weak models. Tuttavia, l’algoritmo XGBoost prevede un miglioramento inerente anche alla velocità di esecuzione ed integra la parallelizzazione. L’idea non è quella di eseguire parallelamente i vari weak models (sennò si tratterebbe della tecnica di bagging), ma di parallelizzare le operazioni di costruzione per ogni singolo albero utilizzando openMP per creare rami in modo indipendente. Questo è reso possibile grazie al fatto che l’inner loop e l’outer loop di costruzione dell’albero sono intercambiabili tra loro. Più nello specifico, il ciclo esterno elenca i nodi foglia di un albero, mentre il ciclo interno si occupa del calcolo delle features. Questa parallelizzazione aumenta significativamente le prestazioni dell’algoritmo.
    \item \textbf{Utilizzo ottimizzato dell’hardware:} L’algoritmo fa buon uso delle risorse hardware avendo consapevolezza della cache, cioè, alloca dei buffers interni per ciascun thread per immagazzinare le statistiche sul gradiente.
    \item \textbf{Regolarizzazione:} Al fine di prevenire l’overfitting, XGBoost corregge i modelli più complessi implementando sia la regolarizzazione di tipo Lasso (L1) che di tipo Ridge (L2).
    \item \textbf{Cross Validation:} L’algoritmo ha un metodo di cross validation incorporato in ogni iterazione; dunque, non risulta necessario programmarla e specificare il numero esatto di iterazioni di boosting richieste in un’unica esecuzione.
    \item \textbf{Gestione dei missing data:} XGBoost è in grado di gestire automaticamente i dati mancanti durante l'addestramento, semplificando il processo di preparazione dei dati e migliorando la robustezza del modello.
\end{itemize}
Altre caratteristiche importanti riguardano invece:\bigskip
\begin{itemize}
    \item Loss function personalizzabile: XGBoost consente agli utenti di definire e utilizzare Loss functions personalizzate, fornendo una flessibilità maggiore nell'adattamento dell'algoritmo a specifici problemi.
    \item \textbf{Gestione dei valori anomali:} XGBoost è più robusto alla presenza di valori anomali, gestendoli in modo più efficace rispetto ad alcune implementazioni di Gradient Boosting.
\end{itemize}
\section{Iperparametri di XGBoost}
Come accennato in precedenza, nei modelli di machine learning sono presenti gli iperparametri.\\
Gli iperparametri sono variabili di configurazione esterne che i data scientist utilizzano per gestire la formazione dei modelli di machine learning. Talvolta chiamati “iperparametri del modello”, questi vengono impostati manualmente prima di addestrare un modello.\\
Sono diversi dai parametri, che sono invece parametri interni derivati automaticamente durante il processo di apprendimento e non impostati dai data scientist.\\
Alcuni esempi di iperparametri possono essere il numero di rami di un albero delle decisioni o ad esempio la sua profondità. Gli iperparametri determinano caratteristiche chiave come l'architettura del modello, il tasso di apprendimento e la complessità del modello.\\\\
Concentrandoci però sull’algoritmo XGBoost, esistono 3 categorie di iperparametri:\bigskip
\begin{enumerate}
    \item \textbf{General parameters:} Guidano il funzionamento complessivo/generale del modello XGBoost;
    \item \textbf{Booster parameters:} Guidano il booster individuale in ogni fase;
    \item \textbf{Learning Task parameters:} Definiscono l’obiettivo di ottimizzazione, la metrica che deve essere calcolata ad ogni passaggio durante il processo di apprendimento.

\end{enumerate}
\subsection{General parameters}
Tra questi si individuano tre iperparametri principali:\bigskip
\begin{itemize}
    \item booster: Consente di capire quale booster utilizzare e quindi a selezionare quale modello eseguire ad ogni iterazione;
    \item verbosity: Consente di specificare la cadenza di iterazioni con cui i messaggi verranno inviati. Se è impostata su 0 non verranno mostrati messaggi di training;
    \item nthread: Numero di thread paralleli usati per eseguire XGBoost. Si usa per l’elaborazione parallela e va inserito il numero di core da usare. Se si vogliono usare tutti i core si può anche non inserire nessun valore;

\end{itemize}
\subsection{Booster parameters}
Esistono 2 tipi di boosters, ovvero, tree booster e linear booster. Tra questi però il tree booster supera sempre il linear booster, che viene per questo motivo usato raramente. Ci si concentrerà dunque solo sulla descrizione degli iperparametri relativi al tree booster:\bigskip
\begin{itemize}
    \item eta: È analogo al valore del learning rate definito per il Gradient Boosting. Questo consente di ridurre la dimensione del passo attuale utilizzata nell’aggiornamento, per evitare che ci sia un adattamento eccessivo e si finisca dunque in overfitting. Varia infatti nel range [0,1]. Dopo ogni boosting step si ottengono i pesi da associare ai passi successivi e eta ha il compito di ridurli per rendere il processo più conservativo;
    \item gamma: Normalmente uno split di un nodo avviene quando da questo split si ottiene una riduzione positiva della loss function. L’iperparametro gamma specifica la riduzione minima che deve avvenire nella loss affinchè si possa effettuare lo split (ovvero la loss deve essere ridotta di almeno gamma). Questo varia in [0, \infty];
    \item max\_depth: Specifica la massima profondità di un singolo albero e si usa per controllare l’overfitting. Più è alto e più il modello sarà complesso (e tenderà all’overfitting). Il valore 0 definisce il caso in cui non si specifica un limite alla profondità. Range: [0, \infty];
    \item min\_child\_weight: Definisce il valore minimo che si deve avere attraverso la somma dei pesi di tutte le osservazioni richieste in un nodo figlio di un albero. Anche questo serve a controllare l’overfitting, infatti più è alto il valore e più è difficile che il modello si adatti perfettamente in training. Bisogna stare però attenti perché, se il valore diventa troppo alto si rischia di andare in underfitting. Range: [0, \infty];
    \item max\_delta\_step: Questo iperparametro viene utilizzato quando si ha un problema che crea un albero fortemente sbilanciato. Come visto nella teoria, infatti, si ha che $\alpha_t=\frac{(-\sum_{i=1}^N g_{it}+\lambda)}{(-\sum_{i=1}^N g_{it}+\lambda)}$. Nel caso in cui un albero sia sbilanciato, il modello si sbilancia su una decisione in particolare e questo porta l’hessiana a tendere a 0. Ma allora $α_t$ tenderebbe ad infinito. Questo se non fosse però per il termine di regolarizzazione λ che vediamo essere stato introdotto nella formula e che è regolato dall’iperparametro in questione. In questo modo vengono limitati forti squilibri. I valori sono compresi tra: [0, inf];
    \item subsample: Denota la frazione delle osservazioni che vengono casualmente campionate per ciascun albero. Questo limita le situazioni di overfitting e viene effettuato una volta per ogni iterazione del processo di boosting. Valori più bassi limitano l’overfitting, ma valori troppo bassi possono portare all’underfitting. Il range di valori utilizzabili è: (0,1];
    \item colsample\_by: Denota un insieme di tre iperparametri che consentono di campionare le colonne. Il loro range varia tra (0,1] e specificano il numero di colonne che devono essere campionate. Tra i suoi iperparametri ci sono:\bigskip
    \begin{itemize}
        \item colsample\_bytree: È un valore compreso tra 0 e 1 e controlla la frazione di colonne/variabili da campionare durante la costruzione di ciascun albero dell'ensemble. In altre parole, regola la sottomissione delle colonne durante la creazione di ciascun albero. Se ad esempio si avesse un dataset con 10 colonne e si addestrasse un modello XGBoost con questo iperparametro impostato a 0.8, per ogni albero che XGBoost sta costruendo durante l’addestramento selezionerebbe casualmente l’80\% delle colonne. Quindi per la costruzione di quell’albero usa solo quel sottoinsieme casuale di colonne. In questo modo si cerca di introdurre più varietà tra gli alberi, rendendo l’ensemble più robusto e meno incline all’overfitting.
    \item colsample\_bylevel: Controlla la frazione di colonne da campionare per ogni livello dell’albero (ovvero tutti i nodi alla stessa profondità dell’albero);
    \item colsample\_bynode: Controlla la frazione di colonne da campionare durante la costruzione di ciascun nodo dell’albero.

    \end{itemize}

\end{itemize}

\textbf{Esempio di violazione accidentale}:\\
In un call center di un Titolare si verifica un black-out temporaneo, i clienti pertanto non possono chiamare o accedere alle proprie registrazioni.
\\
\\
\textbf{Esempio di violazione illecita}:\\
Un Titolare gestisce dei servizi on line. A seguito di un attacco informatico, i dati personali degli interessati sono trasferiti illecitamente (data extrusion).
\\
\\
Si possono distinguere tre macro-categorie di data breach introdotte dal WP29 (Comitato Europeo della protezione dei dati (EDPB), introdotto dal nuovo regolamento europeo GDPR) \cite{e1} :\smallskip
\\
\begin{itemize}
    \item \textbf{Confidentiality Breach} ("Violazione della riservatezza")\\ In caso di divulgazione o accesso accidentale/abusivo ai dati personali;
    \item \textbf{Availibility Breach} (“Violazione della disponibilità”)\\ In caso di perdita/distruzione accidentale o non autorizzata ai dati personali;
    \item \textbf{Integrity Breach} (“Violazione dell’integrità”)\\ In caso di modifica non autorizzata o accidentale ai dati personali;
\end{itemize}\bigskip
A seconda dei casi una violazione può riguardare contemporaneamente la riservatezza, l’integrità e la disponibilità dei dati personali, nonché qualsiasi combinazione delle stesse.
\\
\\
Con la digitalizzazione delle informazioni sempre più presente nella società, la violazione dei dati è diventata sempre più diffusa. IBM ha calcolato che nel 2014 le aziende avevano il 22.6\% di probabilità di subire un data breach nel giro di due anni, mentre nel 2019 è aumentata al 29.6\% e sta continuando ad aumentare. \cite{e2}
\\
\\
Di seguito vengono analizzate le principali cause che generano perdite di dati.
\\
\section{Cause ed effetti}
\medskip
Secondo il report di Verizon “2018 Data Breach Investigations Report” \cite{e4} ci sono 6 principali fonti di violazione di dati. Tali situazioni si verificano con frequenze diverse, e considerandole da quelle meno frequenti a quelle più frequenti si individuano:
\begin{enumerate}\bigskip
\medskip
    \item \textbf{Azioni fisiche ( 4\% )}\\ \\Solitamente si pensa che le violazioni dei dati siano il risultato di crimini informatici, ma molto spesso accade che, come ha osservato Verizon, un numero significativo di incidenti non coinvolge affatto la tecnologia. Questi incidenti sono ad esempio relativi al furto o allo smarrimento di dispositivi fisici come telefoni, laptop e dispositivi di archiviazione. Tali eventi sono favoriti dall’uso sempre più frequente di tecnologie portatili che consentono ad esempio di lavorare non necessariamente stando fisicamente in un ufficio, ma anche stando a casa o in viaggio. Un’altra azione fisica è quella relativa al card skimming, che è l’azione con cui i truffatori inseriscono un dispositivo skimmer che consente di leggere ed immagazzinare in una sua memoria i dati relativi alla banda magnetica della carta utilizzata nei lettori di carte e negli sportelli automatici.
    
    \item \textbf{Utilizzi non autorizzati (8\%)}\\ \\
    Verizon ha scoperto che più di una violazione dei dati su dodici è causata nelle aziende da dipendenti che usano in modo improprio le informazioni che hanno a disposizione. Il primo modo con cui ciò avviene è quello in cui i dipendenti di un’azienda abusino delle informazioni a cui hanno avuto accesso legittimamente. Potrebbero farlo appositamente per dei loro scopi, ma non sempre ciò viene fatto necessariamente di proposito per scopi dannosi. Spesso accade che l’azienda non protegga correttamente certe informazioni e che i dipendenti in maniera accidentale accedano, condividano o perdano tali informazioni.
    
    \item \textbf{Malware (17\%)}\\ \\
    I malware sono strettamente legati alle attività dei cyber criminali e possono essere utilizzati per molti tipi di illeciti. Un esempio di malware è il RAM Scraper che scansiona la memoria dei dispositivi digitali per raccogliere informazioni sensibili. Un altro esempio è anche quello relativo al reperimento di credenziali delle carte di credito usate agli sportelli bancari mediante determinati strumenti installati abusivamente, come ad esempio i keylogger che catturano la sequenza di tasti premuti su una tastiera e che sono usati per rubare password ed altre informazioni sensibili. Questo caso verrà considerato come esempio dal punto di vista giuridico nella seconda parte di questa tesi. Un’altra forma di malware è quella dei ransomware, che bloccano i dispositivi elettronici e con cui i truffatori richiedono il pagamento di un riscatto per lo sblocco.

    
    \item \textbf{Social engineering (22\%)}\\ \\
    Nel campo della sicurezza informatica la social engineering è lo studio del comportamento individuale di una persona al fine di ottenere informazioni utili. In questo caso si considerano pratiche come quella del phishing, in cui i cyber criminali inviano email apparentemente identiche a quelle di aziende importanti (come le Poste) che richiedono spesso di ripristinare i dati relativi al proprio account. Nel fare ciò, gli utenti accedono a portali fasulli inviando involontariamente le loro credenziali. Una pratica simile è quella del pretexting, che oltre ad essere attuata tramite email avviene anche per telefono. In questo caso i truffatori non creano una copia del sito di un’azienda importante, ma si fanno comunicare direttamente le informazioni finanziarie.\\

    
    \item \textbf{Errore umano (22\%)}\\ \\
    Verizon, come già accennato, ritiene che molte perdite di dati avvengano per errori che vengono commessi dai vari utenti. In particolare si potrebbe pensare al caso in cui ci sia l’invio di informazioni riservate e dati sensibili al destinatario sbagliato. Un’altra situazione legata all’errore umano è quella di un’errata configurazione di database, i quali contengono informazioni sensibili online ma che non sono protette da alcuna password.

    
    \item \textbf{Criminal Hacking (45\%)}\\ \\
    La causa principale di perdita di informazioni è legata invece ad attività criminali. Gli hacker utilizzano diversi stratagemmi per entrare in possesso di determinate informazioni. Tra questi ad esempio c’è l’acquisto di credenziali sul dark web o l’utilizzo di software che consentono la generazione automatica di password. Una volta che un cyber criminale entra in possesso delle credenziali di un utente può usarle in molti modi e nella maggior parte dei casi le usa per commettere determinati crimini, per navigare sul dark web o per lanciare altri attacchi a sistemi aziendali.
\end{enumerate}
\bigskip
\section{Impatto del COVID-19 sui Data Breach}
\medskip
\cite{e5} L’attuale situazione della pandemia del COVID-19 ha generato nuove sfide legate alla sicurezza informatica, nate con l’introduzione del lavoro a distanza. Questa tipologia di lavoro è stata adottata in molti settori lavorativi ed ha introdotto in molte aziende, come ad esempio in quelle di ristorazione o in quelle sanitarie, lo sviluppo di servizi online che hanno consentito la loro continuità lavorativa, favorendo una richiesta molto elevata da parte dei compratori legata alla maggiore disponibilità di prodotti ed alla maggiore velocità di acquisto, limitando così grosse perdite economiche.\\
Purtroppo però questa situazione ha favorito un aumento del numero di crimini informatici e di violazioni di dati che si sono verificate. Secondo uno studio di Verizon, che ha analizzato i vari attacchi che si sono verificati, durante la pandemia sono state usate molte tipologie di attacchi che già esistevano e risultavano funzionanti nella fase pre-covid 19 invece di mobilitarsi verso lo sviluppo di nuove tecniche. \cite{e6} Analizzando 474 casi che si sono verificati tra il 1 marzo 2020 e il 1 giugno 2020, 36 di queste violazioni erano direttamente correlate alla pandemia COVID-19.
\\
\\
Sono state identificate varie minacce comuni tra cui:
\begin{itemize}\bigskip
    \item Continuo incremento di errori umani: Come già accennato in precedenza l’errore umano è una delle principali cause degli incidenti di sicurezza. Questo incremento è dovuto ad esempio alla situazione in cui molte persone lavorando o studiando da casa sono soggette a più tipi di distrazioni rispetto a quando si trovano all’interno di uffici aziendali, ma è anche legato alle continue interruzioni lavorative che si sono verificate in questo periodo di pandemia;
    \item Incremento di tecniche usate dagli hacker per rubare o forzare credenziali: Questo, come è stato evidenziato nella pubblicazione DBIR 2020 di Verizon \cite{e7}, ha caratterizzato l’80\% delle violazioni che si sono verificate ed in questo caso è stato notato anche un aumento dell’utilizzo di ransomware. Dei 474 incidenti accennati prima è stato visto che 128 erano legati a malware e 36 sono state violazioni ransomware.
    \item Utilizzo delle email di phishing: Questo tipo di email venivano inviate spesso anche prima del COVID-19. Un utente indotto a cliccare su un link dannoso o ad aprire un allegato infetto fornirà all’aggressore l’accesso iniziale al sistema considerato. L’attacco può inoltre estendersi maggiormente nel caso in cui l’utente utilizzi la stessa password per più sistemi. È stato però notato che le email correlate al COVID-19 ovvero quelle contenenti alcune parole chiave tra cui “COVID”, “CORONAVIRUS”, “quarantena”, “vaccino”, “mascherine” ed altri termini legati alla pandemia vengono cliccate con una percentuale di click del 4.1\% la quale è leggermente superiore a quella di email non correlate al covid che è del 3.1\%. Per alcune organizzazioni però la percentuale di click per messaggi legati al covid è molto più elevata e sale a più del 50\%.
\end{itemize}
\begin{figure}[ht]
\centering
\includegraphics[scale=1]{img/hold_out.jpg}\\
\caption{Percentuale di click prima e dopo il COVID-19}
\end{figure}\newpage
In Figura 1 ogni punto nel grafico rappresenta il 2\% delle organizzazioni e, mentre nel primo grafico (rappresentante la fase pre-covid) la maggior parte delle aziende non era propensa o raramente cliccava sui link dannosi, nel secondo grafico (periodo pandemico) si può osservare come i vari punti si sono spostati verso un aumento della percentuale di click. Ad esempio, quasi la metà delle aziende che inizialmente presentavano una percentuale dello $0$\% di click nel primo grafico, mostrano un valore percentuale superiore nel secondo grafico.
\\
\\
Questi risultati si ottengono perché al successo che hanno normalmente le tecniche di phishing si aggiungono l’incertezza, la paura e la necessità di avere informazioni riguardanti la pandemia in corso.\\
Una simulazione di phishing effettuata a fine marzo 2020 eseguita su circa 16000 persone ha mostrato che rispetto alla situazione pre-pandemica, non solo c’è stato circa il triplo del numero di persone che hanno cliccato sul link di phishing ma molti hanno anche fornito le proprie credenziali alla pagina di accesso utilizzata nella simulazione. 
\\
I criminali informatici, basandosi sul tema del COVID-19 hanno molte più chance di ottenere le informazioni a cui sono interessati.
\\
\section{Conseguenze di un Data Breach}
\bigskip
Tra le conseguenze \cite{e8} portate da un data breach in ambito aziendale si trovano:\bigskip
\\
\\
\textbf{Danno reputazionale:}
\\
Una volta persi i dati dei propri clienti, la fiducia e la credibilità che erano state acquisite con gli anni vengono perse e ciò non è un danno solo relativo a clienti persi, ma lo è anche per quelli che sarebbero potuti diventare dei nuovi clienti che, vista la situazione, non riporranno la loro fiducia nell’azienda.
\\
\\
\textbf{Risarcimento dei danni agli interessati:}
\\
In questo caso viene considerato l’articolo 82 del GDPR, il quale dice che “Chiunque subisca un danno materiale o immateriale causato da una violazione del presente regolamento ha il diritto di ottenere il risarcimento del danno dal titolare del trattamento o dal responsabile del trattamento”.\\
In questo caso viene data importanza al cliente (considerato parte debole) del quale sono stati persi i dati, e gli viene garantito un risarcimento del danno subìto.
\\
\\
\textbf{Indagini sull'incidente:}
\\
Indagini rivolte a comprendere come si è verificato il data breach e quali sono i danni che ha portato nell’azienda. Spesso queste indagini comportano l’assunzione di personale aggiuntivo che se ne occupi e comportano costi e attività extra rispetto a quelle sostenute normalmente dell’azienda.
Siccome gli impatti di una violazione possono essere estremamente dannosi sia per le vittime (alle quali appartengono le informazioni violate) sia per le imprese, è importante definire un modello organizzativo \cite{e9} ben strutturato che affronti con profitto le tematiche relative a tre punti:
\\
\begin{itemize}\medskip
    \item Valutazione dei rischi;
    \item Organizzazione aziendale;
    \item Sicurezza informatica;
\end{itemize}\bigskip
\textbf{Reiterazione del data breach:}
\\
Una volta risolto il data breach però non è detto che la questione sia conclusa. In particolare potrebbe verificarsi la reiterazione della violazione che potrebbe avvenire in quanto l’hacker riesca a nascondere le sue tracce dopo aver studiato nel dettaglio le varie vulnerabilità dell’azienda. Questo avviene perché inizialmente l’hacker, una volta entrato nel sistema da violare, non attacca subito ma attende un lungo periodo in cui pianifica l’attacco e si prepara. In seguito a questa pianificazione, l’azienda potrà limitare i danni ma è difficile che riesca a risolvere completamente la violazione avvenuta eliminando del tutto anche le tracce dell’hacker. In questa situazione infatti è molto probabile che l’azienda subisca ulteriori danni in futuro.
\\
\section{Prevenire un Data Breach}
\bigskip
Come già discusso, quando un data breach si verifica questo porta varie conseguenze negative. È importante saper rimediare ai danni dopo la violazione, ma questo lavoro in molti casi può essere risparmiato grazie ad un’adeguata fase di prevenzione.\\
Alcune misure preventive che un’azienda può adottare \cite{e11} sono:
\\
\begin{itemize}\medskip
    \item \textbf{Investimenti in formazione:}\\
    Il GDPR impone di investire nella formazione del personale riguardo a queste tematiche. È importante che  l’azienda si occupi di definire politiche accurate sull’uso dei vari dispositivi utilizzati e fare in modo che queste vengano rispettate correttamente effettuando un costante monitoraggio. È poi necessario fornire un’adeguata formazione ai dipendenti che si occupano del trattamento di dati non solo in fase di assunzione, ma in maniera periodica per far sì che siano costantemente aggiornati su regole e procedure relative al tema. Questo può avvenire tramite corsi dedicati alla sicurezza.
    \item \textbf{Monitoraggio dei log:}\\
    I file log contengono un elenco cronologico delle varie attività che vengono svolte da un sistema operativo, da un database o da altri programmi, consentendo una successiva verifica. Questi file sono importanti da monitorare soprattutto per aziende che utilizzano molti server, programmi e applicazioni, in quanto consentono di capire tempestivamente cosa sia successo o cosa stia succedendo in tempo reale. Chi cerca di violare un sistema informatico lascia molte tracce e queste possono essere intercettate mediante l’uso dei log. Tramite il monitoraggio di questi file si riesce a:\\
    \begin{itemize}\smallskip
        \item Determinare se un problema è da considerare effettivamente una vulnerabilità;
        \item Migliorare l’analisi, la riproduzione e la risoluzione dei problemi;
        \item Aiutare a testare nuove caratteristiche dei sistemi;
    \end{itemize}
    \item \textbf{Monitoraggio dei dispositivi finali (endpoint):}\\
    Con l’introduzione dello smart working le aziende hanno a che fare con sempre più endpoint e ciò aumenta l’estensione della rete aziendale. Questo comporta una crescita della “superficie di attacco” da parte di cyber criminali, quindi più è il numero di endpoint e maggiore sarà la probabilità che vengano compromessi i dati aziendali. Gli endpoint dovranno essere costantemente monitorati tramite specifici programmi che verifichino costantemente il loro corretto funzionamento e segnalino un’eventuale presenza di anomalie verificatesi.
\end{itemize}\bigskip
Un’\textbf{osservazione importante} da fare però è quella per cui il budget utilizzato dall’azienda non dovrà essere rivolto solo alla fase di prevenzione di un data breach, ma è importante che questo venga organizzato adeguatamente per garantire che ci siano risorse necessarie anche alla situazione in cui si verifichi il data breach. È importante monitorare costantemente la situazione e gestire al meglio tutte le risorse a disposizione, è però fondamentale essere preparati ad un attacco che può avvenire in qualsiasi momento. La prevenzione infatti consente di limitare i danni, ma la violazione potrà comunque verificarsi ed è importante essere sempre pronti a gestirla.\\
\section{Come comportarsi quando si verifica un Data Breach}
\bigskip
Per spiegare al meglio questa situazione introduciamo alcune figure fondamentali coinvolte \cite{e12}:
\\
\\
\textbf{L'Interessato}, è il soggetto al quale appartengono le informazioni che vengono trattate ed accetta di affidare al Titolare del trattamento i propri dati consapevole dei limiti entro i quali li userà;
\\
\textbf{Il Titolare del trattamento}, è la persona fisica o giuridica a cui spettano singolarmente o di concerto con un altro Titolare, tutte le decisioni in merito alle finalità e modalità del trattamento, compresi i profili inerenti alla sicurezza. È quindi la figura che decide in che modo e a quale scopo potranno essere trattati i dati dell’Interessato;
\\
\textbf{Il Responsabile del trattamento} invece è la persona fisica, la persona giuridica, la pubblica amministrazione e qualsiasi altro ente, associazione od organismo, che possono essere preposti dal Titolare al trattamento dei dati. Il Responsabile è quindi la figura designata dal titolare per operare materialmente con i dati personali da lui raccolti, secondo le finalità comunicate all’interessato.
\\
\\
Qualora dovessero essere violati dei dati \cite{e10}, la prima cosa che il Titolare del trattamento di tali dati deve fare è quella di notificare tale situazione al Garante privacy il prima possibile, ovvero entro le 72 ore immediatamente successive a quando ne è venuto a conoscenza, senza ingiustificato ritardo (un eventuale ritardo dovrà essere motivato).
\\
\\
Tale notifica contiene\cite{e11}:\bigskip
\begin{itemize}
    \item Una descrizione della violazione e del numero di interessati coinvolti;
    \item I nomi e i contatti del Data Protection Officer (DPO), che è la figura professionale che presta consulenza in materia di privacy e sorveglia l’attività di colui che l’ha nominato (ovvero del Titolare del trattamento);
    \item Una descrizione delle probabili conseguenze in caso di violazione;
    \item Una descrizione delle misure adottate per proteggere i diritti degli interessati e contrastare le conseguenze negative della violazione;
\end{itemize}\bigskip
Se è il Responsabile del trattamento ad accorgersi della violazione deve mobilitarsi per informare tempestivamente il Titolare dei dati in modo che possa attivarsi.\\
Il Titolare deve comunicare l’avvento di tale violazione ai diretti interessati quando questa può comportare un rischio elevato per i loro diritti, a meno che abbia già preso misure tali da ridurne l’impatto. Inoltre il Titolare si occupa anche di documentare tutto ciò che riguarda l’avvenuta violazione in un apposito registro che consentirà alle Autorità di effettuare eventuali verifiche sul rispetto della normativa.
\\
\\
\cite{e24} Si cerca di ridurre il più possibile i rischi e le conseguenze del data breach a cui i dati personali sono stati esposti. Va tenuto in considerazione il fatto che non è possibile eliminare completamente il rischio, ma con un approccio metodologico e organizzativo strutturato si può ridurre la probabilità che si verifichi un data breach e attenuarne le conseguenze.\\
Un processo di analisi del data breach ben strutturato prevede quattro fasi:
\begin{enumerate}\medskip
    \item Preparazione;
    \item Reazione;
    \item Comunicazione;
    \item Registro;
\end{enumerate}\bigskip
Indipendentemente dalle dimensioni e dalla complessità di un’organizzazione è necessario stabilire come procedere nel caso di violazione della sicurezza. Le quattro fasi elencate sono incluse in un processo di gestione del data breach rappresentato nel seguente schema:
\begin{figure}[ht]
\centering
\includegraphics[scale=0.8]{img/img3.jpg}\\
\caption{Processo di Gestione dei Data Breach}
\end{figure}
\\
\\
\textbf{Fase di Preparazione:}\\
La preparazione è il primo step nella gestione di un data breach ed è la fase in cui si identificano i processi di business critici su cui focalizzare l’attenzione e si stabiliscono le soglie di tolleranza al rischio stesso in funzione delle strategie aziendali. L’organizzazione definisce risorse e mezzi necessari per una corretta gestione del data breach, orientando di conseguenza la definizione di budget, personale, piani di formazione del personale e piani implementativi (di gestione del data breach).\\
Vengono poi definiti e descritti i flussi di comunicazione a seguito di una violazione, i ruoli e le responsabilità di ciascuna funzione aziendale, eventuali comitati (di direzione e operativi), strumenti a supporto, le modalità ed i tempi di notifica al Garante e le altre entità istituzionali previste dagli obblighi di legge. Chiaramente più l’organizzazione è complessa e più sarà articolata la gestione di questa fase.
\\
\\
\textbf{Fase di Rilevazione/Identificazione:}\\
In questa fase vanno specificate le situazioni considerate incidenti di sicurezza, gli strumenti, i sistemi di rilevazione ed allarme che il titolare utilizzerà per rilevare la violazione, classificarla ed analizzare le informazioni fornite dagli strumenti stessi. Il momento di rilevazione e identificazione di una violazione è critico, perché il GDPR stabilisce che si debba notificare il Garante entro 72 ore da quando ci si accorge della violazione senza ingiustificato ritardo, ma quando ci si trova in questa fase potrebbe capitare di non avere a disposizione tutte le informazioni necessarie a classificare l’evento di sicurezza come data breach e potrebbe essere necessario spostare l’inizio del conteggio delle 72 ore nella successiva fase di analisi e classificazione dell’incidente. Quando viene classificato un evento come violazione di dati, è bene che questo venga memorizzato in un archivio (registro di data breach) che ne riporti la tipologia, lo stato, la gravità e le misure adottate per la risoluzione sia per fare in modo che situazioni simili, che si verificheranno in futuro, vengano gestite velocemente e sia per avere una visione d’insieme che consenta di correlare più eventi per identificare eventuali azioni strutturate di attacco.
\\
\\
Le fasi successive, ovvero quelle di Analisi/Classificazione, Risposta, Notifica e la fase di Monitoraggio e Chiusura compongono il cosiddetto \textbf{piano d’azione}, che è l’insieme delle attività di diversa natura che concorrono alla gestione del data breach.
\\
\\
\\
\textbf{Fase di Analisi/Classificazione:}\\
Questa fase viene effettuata sulla base di diversi fattori, tra cui:
\begin{itemize}\bigskip
    \item Tipologia della minaccia;
    \item Categoria dei dati interessati;
    \item Utenti interessati dalla minaccia;
    \item Numero e classificazione dei sistemi interessati;
    \item Impatto dell'incidente sull'organizzazione e sui diritti degli interessati;
\end{itemize}\bigskip
Se l’evento è classificato come incidente e vengono compromessi i dati personali, si è in presenza di un data breach. Questa è la fase in cui il titolare, oltre a venire a conoscenza dell’accaduto ha anche gli elementi che gli consentono di classificare l’incidente come data breach valutandone impatti ed azioni di risposta. È questo il momento da cui partono le 72 ore disponibili per notificare al Garante l’accaduto.
\\
\\
\textbf{Fase di Risposta:}\\
Le attività svolte in questa fase sono quelle di:
\begin{itemize}\bigskip
    \item Contenimento dell’incidente mediante azioni di recupero con effetto immediato;
    \item Recupero del funzionamento standard delle funzionalità, dei servizi e dei dati compromessi;
    \item Raccolta e memorizzazione strutturata degli elementi caratterizzanti l’evento e le azioni intraprese;
\end{itemize}\bigskip
Deve essere predisposto un team di persone che si occupi di contenere ed eliminare gli effetti dell’evento che si verifica, nel minor tempo possibile. Vengono identificate le soluzioni da mettere in atto per contenere il rischio di una futura violazione. Poi si definisce la percentuale di rischio che l’azienda è disposta ad accettare. Successivamente si implementano le soluzioni necessarie a contenere la violazione ed una volta applicate si verifica se ci si trova nuovamente nella situazione a regime. Quindi, risolta la violazione, tramite la fase di recupero vengono ripristinati completamente i servizi evitando che in futuro si possano riverificare incidenti legati alla stessa causa.
\\
\\
\textbf{Fase di Notifica:}\\
In questa fase avviene effettivamente la notifica al Garante entro le 72 ore che erano iniziate alla fine della fase di analisi e classificazione. Il GDPR stabilisce che la notifica debba avvenire entro questo periodo temporale a meno che non sia improbabile che la violazione della sicurezza possa rappresentare un rischio per i diritti e le libertà fondamentali delle persone. Nel caso di organizzazioni complesse e di dimensioni rilevanti è cruciale che ruoli, funzioni coinvolte, modalità e tempistiche siano già state stabilite in modo preciso per evitare incertezze anche alla luce degli stringenti tempi in cui deve avvenire la comunicazione al Garante.
\\
\\
\textbf{Fase di Monitoraggio e Chiusura:}\\
Questa è l’ultima fase del piano d’azione e comprende diverse azioni. In questa ci sono azioni legali ed altre di comunicazione agli organi di stampa nel caso in cui ci possano essere ripercussioni su reputazione e/o immagine del soggetto coinvolto e avvengono azioni di monitoraggio periodico che tengono sotto controllo i livelli di rischio. Completate le varie azioni si procede poi alla chiusura effettiva del data breach con un rapporto che ripercorre tutte le attività effettuate in questo processo di gestione del data breach.
\\
\section{Casi pratici}
\medskip
Esistono svariati modi per generare un data breach, abbiamo parlato del phishing in cui avviene l’invio di un’email contenente un link dannoso, che se viene aperto dal destinatario dell’email fornisce un accesso al sistema da parte dell’aggressore. Altre situazioni molto frequenti invece, come viene detto nella pubblicazione di Verizon \cite{e13} “Data Breach Digest” del 2016, sono quelle che riguardano l’utilizzo di conduit devices in cui i mezzi utilizzati per la violazione sono dei dispositivi fisici. Si può pensare al caso già accennato relativo allo skimming, in cui gli aggressori montano negli sportelli bancari un dispositivo detto skimmer posizionato sopra la fessura originale per l’inserimento della carta di pagamento. Quando la vittima inserisce la carta di pagamento all’interno dello skimmer, questo tramite un lettore di banda magnetica copia i dati della carta. In questo tipo di pratica viene poi montata una telecamera nella parte superiore dello sportello bancomat che registra l’introduzione del PIN e la trasmette al truffatore.
\\
\\
Un caso che però viene trattato nella pubblicazione di Verizon è quello della USB Infection.\\
Nel primo periodo in cui i virus informatici hanno cominciato a diffondersi, il principale mezzo usato per infettare i dispositivi era il floppy disk. Oggi la funzione di memorizzazione che prima veniva svolta dai floppy disk viene svolta dalle chiavette USB \cite{e14}, le quali rappresentano una delle principali forme di infezione dei sistemi informatici, in quanto, quando un malware infetta una chiavetta USB riuscirà ad infettare tutti i dispositivi a cui questa si connetterà.
\\
\\
Esistono diversi modi con cui una USB infetta può compromettere la sicurezza di un computer:
\begin{itemize}\bigskip
    \item Potrebbe esserci un malware che consente a qualcun altro di prendere il controllo della webcam, della tastiera o del microfono;
    \item Potrebbero essere cancellate informazioni personali o cancellati i dati dal dispositivo;
    \item In alcuni casi potrebbe anche esserci il danneggiamento dell’hardware;
\end{itemize}\bigskip
Ci sono tre tipologie principali \cite{e19} di attacchi con USB infette:
\begin{enumerate}\bigskip
    \item Il più comune è quello di codice dannoso che viene avviato quando viene cliccato un file della chiavetta che una volta eseguito scaricherà un malware da Internet;
    \item C’è poi quello relativo alla social engineering, in cui l’utente, tramite il tentativo di apertura di un file nella chiavetta USB, viene reindirizzato a un sito di phishing che cerca di fargli inserire delle credenziali per potergliele rubare;
    \item Il terzo tipo di attacco è quello dello Human Interface Device (HID) Spoofing, dove il dispositivo connesso sembra una chiavetta USB, ma in realtà è un dispositivo che induce il computer a credere di essere collegato ad una tastiera e che fornisce ad un hacker l’accesso remoto a tale computer.
\end{enumerate}\bigskip
Uno studio di Microsoft svolto nella prima metà del 2011 ha analizzato oltre 600 milioni di sistemi nel mondo ed ha rilevato che circa il 26\% delle infezioni di malware provenivano dall’uso di chiavette USB infette che sfruttavano la funzione di AutoRun di Microsoft Windows. L’AutoRun \cite{e15} è la funzionalità di alcuni Sistemi Operativi di eseguire automaticamente delle operazioni all’inserimento di un’unità rimovibile (come una chiavetta USB). In Microsoft Windows questa funzionalità opera mediante l’utilizzo del file autorun.inf, nel quale sono specificate le istruzioni che il sistema eseguirà al collegamento della periferica. Tuttavia l’AutoRun è stato molto criticato in ambito di sicurezza in quanto, come è stato visto dallo studio condotto da Microsoft nel 2011, può essere utilizzato per l’installazione automatica di malware sia sul dispositivo utilizzato e sia su altre periferiche che vengono collegate a tale dispositivo.
\\
\\
Un caso molto famoso di USB infection è il caso Stuxnet.\\
Stuxnet \cite{e16} è un virus informatico creato e diffuso appositamente dal governo statunitense in collaborazione col governo israeliano per sabotare la centrale nucleare iraniana di Natanz, disabilitando i sistemi di sicurezza che rilevavano malfunzionamenti o la presenza di virus. Stuxnet colpiva i PLC, che sono componenti hardware e software fondamentali per l’automazione degli impianti della centrale, e ne disabilitava le centrifughe. Per infettare il sistema, il malware si basò su quattro vulnerabilità di Windows che non erano ancora state rilevate per poi propagarsi verso il software Step7 della Siemens, il quale è un pacchetto software per sviluppare progetti di automazione basati sui prodotti di Siemens. Per un errore di programmazione del virus però questo infettò un computer portatile e si propagò (erroneamente) anche al di fuori della centrale nucleare.\\
Stuxnet, a differenza della maggior parte dei malware, era stato progettato per danneggiare solo i sistemi della centrale nucleare, ovvero sistemi con particolari requisiti e agiva solo se rilevava sistemi dotati del software Siemens Step7, altrimenti si disattivava. Il malware aggirò Windows infettando inizialmente un computer della centrale tramite una chiavetta USB e propagandosi poi nella rete industriale tramite una rete peer to peer.\\
Stuxnet pesa mezzo MegaByte di memoria ed è stato scritto utilizzando più linguaggi di programmazione, però il suo codice non è mai stato rivelato. I suoi driver erano firmati con la chiave privata di due certificati rubati alle aziende JMicron e Realtek, e grazie a tali firme i driver del virus sono stati installati nel kernel di Windows senza generare nessun allarme. Sono stati poi infettati i progetti che utilizzavano il software della Siemens WinCC/ PCS7 Step7 per sistemi SCADA (sistemi informatici per il monitoraggio e l’automazione di sistemi fisici), sostituendo una libreria chiave di WinCC e riuscendo a intercettare lo scambio di messaggi tra Windows ed i PLC. In questo modo non venivano rilevati errori nei blocchi di codice dei PLC perché era WinCC ad essere stato infettato.\\
L’infezione dei PLC avvenne con l’uso di convertitori di frequenza che monitoravano le frequenze dei motori infetti e quando venivano soddisfatte una serie di condizioni richieste dal malware, venivano modificate le frequenze di lavoro dei motori infetti facendo però credere agli operatori della centrale che le frequenze di lavoro fossero sempre quelle corrette. In seguito all’incidente, Siemens ha diffuso uno strumento per la rimozione del virus che però non funziona completamente, ma richiede comunque un’attenta analisi dei PLC che sono stati infettati.
\\
\\
Uscendo dal caso Stuxnet esistono inoltre dispositivi dalle sembianze di chiavette USB ma che non contengono dati. Sono quelle che si chiamano USB Killer \cite{e17}, che mandano sovraccarichi di alta tensione nel dispositivo in cui vengono inserite danneggiandone i componenti hardware.
\\
\\
Questo fa comprendere quanto sia pericoloso l’utilizzo di una USB dal contenuto sconosciuto su un certo dispositivo.\\
Quando non si conosce il contenuto di una chiavetta USB quindi è bene non inserirla in nessun dispositivo, questo però non risolve il problema della USB Infection perché anche una nostra chiavetta USB può essere infettata in qualsiasi momento quando viene utilizzata per lo scambio di dati tra più computer quando uno di essi è infetto. Come già detto un dispositivo viene infettato da una chiavetta esterna tramite la funzione di Autorun col file autorun.inf ed una soluzione offerta da Windows 10 è quella di disabilitare l’Autorun \cite{e18} dalle impostazioni. Questa soluzione però risolve solo in parte il problema perché se la chiavetta contiene file infetti, accedendoci manualmente, c’è comunque la possibilità che il computer venga infettato. Una soluzione consigliabile è quella di utilizzare buoni antivirus che esaminino i file della chiavetta prima dell’attivazione dell’AutoRun e che magari offrano soluzioni per evitare che l’Autorun possa essere utilizzato a scopo malevolo.
\\
\\
Un altro scenario molto diffuso analizzato dalla pubblicazione “Data Breach Digest” di Verizon del 2016 \cite{e13} è quello che si concentra maggiormente sui malware. In particolare viene analizzato lo scenario relativo ai ransomware che Verizon definisce come uno scenario letale nell’ambito dei data breach.
\\
\\
Il ransomware \cite{e20} è un tipo di malware che può infettare un dispositivo limitandone l’accesso a tutti o ad alcuni dei suoi contenuti richiedendo successivamente un riscatto per sbloccarli.
\newpage
Esistono due tipologie principali di ransomware:
\begin{itemize}\bigskip
    \item \textbf{Cryptor:} Criptano i file contenuti nel dispositivo rendendoli inaccessibili;
    \item \textbf{Blocker:} Bloccano l’accesso al dispositivo infettato;
\end{itemize}\bigskip
Il Rapporto Clusit 2021 ha stimato che nel 2018 i ransomware rappresentavano il 23\% di tutti i malware; nel 2019 sono saliti a 46\% e nel 2020 sono arrivati al 67\%. Quando un ransomware infetta un computer, al posto del solito sfondo comparirà un avviso che richiede il pagamento di un riscatto in cambio di una password che sia in grado di sbloccare il computer. La richiesta di denaro è relativa ad una somma che inizialmente era sotto i 1000 dollari, ma negli ultimi anni è cresciuta molto fino a milioni di dollari, richiesti solitamente in criptovaluta.
\\
\\
La modalità più diffusa con cui un ransomware infetta un computer è quella delle email di phishing (sfruttano il social engineering) che invitano a cliccare su un link dannoso o a scaricare un determinato file \cite{e21}. Di solito usano lo “spoofing” per mascherare il loro indirizzo e fare in modo che l’indirizzo visualizzato sia di una persona conosciuta dalla vittima o di una persona che sembra affidabile. Questo avviene solitamente mediante l’uso di email, sms o sistemi di messaggistica. Oltre al phishing, altre situazioni che portano il dispositivo ad essere infettato sono:
\begin{itemize}\bigskip
    \item La navigazione su siti compromessi: In questo caso la situazione varia rispetto a quella del phishing, perché i cybercriminali infettano il sito ma non sono loro a sollecitare la visita delle vittime, le quali vengono indirizzate verso tale sito solitamente tramite link o banner pubblicitari su siti web o su social network;
    \item L’utilizzo di un supporto rimovibile infetto: come già discusso prima per il caso della USB Infection. Spesso la diffusione di questi supporti avviene mediante la tecnica detta “baiting” con la quale il supporto viene lasciato incustodito da qualche parte e fa leva sulla curiosità delle persone che molte volte raccolgono il supporto e lo inseriscono in un loro dispositivo;
    \item Lo scaricamento di determinati software: spesso si trovano in programmi gratuiti che consentono di crackare software costosi per utilizzarli senza pagare. Ciò che verrà installato sarà un eseguibile (.exe) tramite il quale può essere infettato il dispositivo utilizzato;
    \item Attacchi mediante desktop remoto: Sono attacchi con furto di credenziali. Una volta che il cybercriminale riesce ad accedere al sistema può commettere varie operazioni tra cui anche furto di credenziali e iniezione del ransomware;
\end{itemize}\bigskip
Ci sono vari modi per prevenire i ransomware:
\begin{itemize}\bigskip
    \item Bisogna sempre essere attenti ed aggiornare costantemente sia il proprio antivirus che il proprio sistema operativo;
    \item È importante utilizzare dei sistemi di backup che salvino una copia dei dati. Questa operazione deve essere effettuata regolarmente ed è importante che i dati vengano copiati più volte e su sistemi diversi. In questo modo, se il ransomware dovesse infettare il sistema, una copia dei dati rimarrebbe protetta dando la possibilità di ripristinarli quando necessario, quantomeno fino all’ultimo salvataggio che era stato fatto;
    \item È sempre bene evitare l’uso di cartelle condivise in reti pubbliche;
\end{itemize}\bigskip
Qualora il dispositivo dovesse essere infettato da un ransomware bisogna valutare attentamente le opzioni a disposizione. Pagare il riscatto non garantisce di riavere indietro i dati persi ma potrebbe comunque essere un’opzione da prendere in considerazione qualora tali informazioni siano molto importanti, ad esempio nel caso in cui sia stato infettato un intero sistema aziendale \cite{e22}. Solo il 20\% dei paganti riesce ad ottenere il ripristino dei dati e talvolta solo parzialmente. In caso si decida di pagare il riscatto si dovranno seguire le istruzioni che vengono impartite nella schermata che compare quando si accede al sistema e il pagamento avverrà in criptovalute. Un’alternativa potrebbe essere quella di contattare esperti informatici per ripristinare il sistema, spesso però i costi richiesti per il ripristino in tal caso sono di qualche ordine di grandezza superiore rispetto a quelli richiesti dai cybercriminali. Va fatta quindi un’attenta valutazione costi/benefici tra pagare (e sperare) e tentare autonomamente di recuperare i dati. Inoltre la decisione da prendere deve essere valutata rapidamente in quanto il riscatto dovrà essere pagato entro un tempo limite impostato dai cybercriminali (che solitamente è di 72 ore). Una soluzione che però non è sempre funzionante e che può essere attuata solo quando il sistema non è bloccato, ma si tratta di un ransomware cryptor, è quella di utilizzare un decryptor ossia un software capace, nei casi in cui la versione del ransomware sia poco efficiente, di recuperare i file crittografati. Nella scelta del decryptor non ci si deve far prendere dalla fretta e va prestata molta attenzione ai falsi decryptor che potrebbero criptare nuovamente i dati precedentemente già criptati.\\
Un’altra soluzione che invece si può prendere in considerazione, se possibile, è quella di non pagare il riscatto e neanche tecnici specializzati, perdendo definitivamente i dati sul dispositivo. Quest’ultima soluzione è difficile che venga presa in considerazione da un’azienda, ma se i dati non dovessero essere di grande importanza può rivelarsi la soluzione migliore.\\
È sempre consigliabile segnalare o denunciare l’attacco alla Polizia Postale aiutando a prevenire ulteriori illeciti.
\\
\\
Un esempio di ransomware molto diffuso è CryptoLocker che è comparso alla fine del 2013 ed è stato perfezionato nel 2017.\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.51]{img/img2.jpg}\\
\caption{Schermata che compare quando il ransomware CryptoLocker infetta un dispositivo}
\end{figure}\\
Questo ransomware cripta i dati dei sistemi Windows e richiede un riscatto alla vittima per decriptarli. Symantec ha stimato che circa il 3\% delle vittime decide di pagare, ma come già detto in precedenza non c’è la garanzia di poter riaccedere ai dati.\\
Nella maggior parte dei casi CryptoLocker si diffonde tramite la posta elettronica, con un messaggio che sembra inoffensivo, tramite un file ZIP allegato alla email contenente un file eseguibile con un’icona e un’estensione pdf nel nome (quindi avrà un nome del tipo nomefile.pdf.exe) avvalendosi del fatto che Windows non mostra di default l’estensione dei file. Inizialmente il software si installa nella cartella Utenti di Windows con un nome casuale e mette una chiave al registro che lo avvia automaticamente. Tenta poi di connettersi ad uno dei server di comando e controllo ed una volta che ci riesce genera una chiave RSA a 2048 bit mandando la chiave pubblica al computer infetto. Successivamente il malware inizia a cifrare i file del disco rigido e cifra solo i dati con determinate estensioni e caratteristiche. Il software informa quindi la vittima di aver cifrato i suoi file e richiede un pagamento di 300 Dollari (o Euro) oppure di 0.5 Bitcoin per decifrare i file. Il pagamento deve essere eseguito entro 72 o 100 ore e fornisce all’utente un software di decifratura con la chiave privata già precaricata, altrimenti se tale riscatto non viene pagato la chiave privata verrà cancellata e nessuno sarà più in grado di decifrare quei file. \cite{e23} \\
I ricercatori ritengono che anche nel caso in cui il malware venisse subito rimosso non sarebbe possibile decriptare i file in nessun modo. Alcuni esperti ritengono che l’unico modo per ottenere nuovamente i propri file sia quello di pagare il riscatto in quanto la chiave utilizzata per la cifratura è molto lunga e sarebbero inutili i tentativi di recupero nel tempo che si ha a disposizione prima che l’unica chiave di decifratura venga cancellata.\\ Inizialmente, verso la fine del 2013, c’era la possibilità di recuperare la chiave anche dopo la scadenza dei termini pagando però un costo aumentato di 10 bitcoin (all’epoca erano 3500 Dollari).
\\
\\
Per prevenire attacchi ransomware, oltre alle normali regole di prevenzione già discusse in precedenza, è bene considerare anche la cartella AppData del computer. Siccome spesso i file vengono salvati all’interno della cartella AppData, una tecnica utile per prevenire attacchi è quella di disabilitare l’esecuzione dei programmi nella directory AppData e nelle sue sottodirectory. Si vuole quindi fare in modo che file eseguibili (.exe) non vengano mandati in esecuzione, e per fare ciò vengono definite delle regole che disabilitano la loro esecuzione.\\
Tra queste si trovano per esempio:
\\
\\
\\
\begin{tabular}{lp{0.5\textwidth}}
\toprule
\textbf{\%AppData\%\textbackslash *.exe} & che previene l’esecuzione dei programmi in AppData \\
\midrule
\textbf{\%AppData\%\textbackslash *\textbackslash *.exe} & che previene l’esecuzione dei programmi nelle sottodirectory di AppData \\
\midrule
\textbf{\%LocalAppData\%\textbackslash Temp\textbackslash XXXX*\textbackslash*.exe} & previene l’esecuzione di programmi estratti automaticamente da file compressi del tipo indicato al posto di XXXX scaricati tramite email. Al posto di XXXX ci possono essere ad esempio: Rar, 7z, wz, .zip \\
\bottomrule
\end{tabular}